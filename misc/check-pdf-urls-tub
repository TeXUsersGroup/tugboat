#!/bin/sh
# $Id$
# Extract urls from a pdf and try visiting them.

for f in "$@"; do
  # We exclude adobe.com since it does not respond to wget requests.
  # From Max Chernoff, 31jul2024.
  #With wget v1.xx (which is what the tug.org server is using):
  qpdf --qdf "$f" - \
  | grep -avF example.org \
  | grep -avF adobe.com \
  | grep -aoP '(?<=/URI \()[^)]*(?=\))' \
  | sort -u \
  | wget --timeout=10 --tries=0 --spider -nv -i- 2>&1 \
  | tee /tmp/ckurl \
  | sed -e 's/^.* URL: //' -e 's/:$//'
done
# output looks like:
#  2024-08-03 08:55:39 URL: https://github.com/borisveytsman/bookshelf 200 OK
# or on error:
#  https://www.perplexity.ai/:
#  Remote file does not exist -- broken link!!!
# which is why we remove trailing colons in the last sed.

#With wget v2.xx (which is what I have installed locally), try:
#   qpdf --qdf /path/to/file.pdf - | grep -aoP '(?<=/URI \()[^)]*(?=\))' | wget --spider -i- | grep ^HTTP

# Basic idea for checking urls in html:
# @links = $html =~ m/<a[^>]+href\s*=\s*["']?([^"'> ]+)/ig;
